{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Hdix7MYPik"
      },
      "source": [
        "# Machine Perception Assignment\n",
        "Please note: for all proper referencing of the code adapted / used in this notebook, please scroll to the bottom "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2HqPs0eY1Vj"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zwSW2BfY2qU"
      },
      "outputs": [],
      "source": [
        "#Code to connect your google drive with google colaboratory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#Importing some useful packages\n",
        "\n",
        "import numpy as np #Numpy library provides various useful functions and operators for scientific computing\n",
        "import cv2 as cv #openCV is a key library that provides various useful functions for computer vision\n",
        "import os #Honestly this one is a bit optional. \n",
        "import glob #again just optional\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab.patches import cv2_imshow "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFMfhvUUknl9"
      },
      "source": [
        "Image processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUYmyjvpP4iF"
      },
      "outputs": [],
      "source": [
        "# Runs a set of image processing (Gaussian blur + Thresholding) and returns processed image\n",
        "def image_process_set_1(img):\n",
        "  gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "  blurred = cv.GaussianBlur(gray, (9,9), 0)\n",
        "  threshold = cv.adaptiveThreshold(blurred, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV, 25, 9)\n",
        "  #cv2_imshow(threshold)\n",
        "  return threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAfhwMM3w_tO"
      },
      "outputs": [],
      "source": [
        "# Runs a set of image processing (Gaussian blur + Thresholding) and returns processed image\n",
        "def image_process_set_2(img):\n",
        "  gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "  #cv2_imshow(cv.bitwise_not(gray))\n",
        "  blurred_inv = cv.GaussianBlur(cv.bitwise_not(gray), (3,3), 0)\n",
        "  threshold = cv.threshold(blurred_inv, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU)[1]\n",
        "  #cv2_imshow(threshold)\n",
        "  return threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1plMWqyksRb"
      },
      "source": [
        "Sign detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6XnwqM3cO3z"
      },
      "outputs": [],
      "source": [
        "# Detects the black box region around the numbers through MSER\n",
        "# This method has been modified and retrieved from Practical 3 (Retrieve Blobs)\n",
        "def detect_number_sign(processed_img, og_img):\n",
        "  mser = cv.MSER_create(max_area=15000, min_area=1000) #Reduce amount of regions detected via area\n",
        "  regions, _ = mser.detectRegions(processed_img) #Detects all regions in the processed image\n",
        "  #Finds the \"best\" rectangular/box region out of all detected regions that would match the black area around numbers\n",
        "  best_ratio = find_closest_sign_ratio(regions) \n",
        "  return return_number_sign(best_ratio, regions, og_img)\n",
        "\n",
        "#Finds the closest ratio to what building sign could be\n",
        "def find_closest_sign_ratio(regions):\n",
        "  #Obtained through trial and error of MSER detected regions ratios on train set\n",
        "  number_sign_ratio = 0.566 #Best medium (Out of all images, this seems to be the best ratio value to capture to signs)\n",
        "  ratios = []\n",
        "  #img = img.copy()\n",
        "  for p in regions: #Grabs all ratios of each detected region\n",
        "      xmax, ymax = np.amax(p, axis=0)\n",
        "      xmin, ymin = np.amin(p, axis=0)\n",
        "      h = ymax - ymin #Height distance\n",
        "      w = xmax - xmin #Width distance\n",
        "      ratios.append(h/w)\n",
        "      #cv.rectangle(img, (xmin,ymax), (xmax,ymin), (0, 255, 0), 3)\n",
        "      #cv.putText(img,str((h/w)),(xmin,ymax), cv.FONT_HERSHEY_SIMPLEX, 0.5,(255,255,255),1,cv.LINE_AA)\n",
        "  #cv2_imshow(img)\n",
        "\n",
        "  #This is the most crucial part of detecting the region as it finds the best region based on ratio for the building sign\n",
        "  #First finds the absolute difference of all the found ratios of detects regions\n",
        "  #Then selects the smallest diff out of all diffs since closest to 0.6 is a sign\n",
        "  best_ratio = min(ratios, key=lambda cur_ratio:abs(cur_ratio - number_sign_ratio))\n",
        "  \n",
        "  return best_ratio   \n",
        "\n",
        "#Goes through all the regions and finds the region with best ratio\n",
        "def return_number_sign(best_ratio, regions, og_img):\n",
        "  for p in regions:\n",
        "      xmax, ymax = np.amax(p, axis=0)\n",
        "      xmin, ymin = np.amin(p, axis=0)\n",
        "      h = ymax - ymin #Height distance\n",
        "      w = xmax - xmin #Width distance\n",
        "      ratio = h/w \n",
        "\n",
        "      if(ratio == best_ratio): #Once we find the best region\n",
        "          cv.rectangle(og_img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 1)\n",
        "          img_region = og_img[ymin:ymax, xmin:xmax] #Crop only to region\n",
        "\n",
        "  return img_region #Return the \"best\" region that we think is the sign "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyDe7yv21gDU"
      },
      "source": [
        "Digits detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHfDK2gd7oKB"
      },
      "outputs": [],
      "source": [
        "# Retrieves digits / numbers from obtained number sign area from previous step\n",
        "def get_digits(sign_img):\n",
        "  threshold = image_process_set_2(sign_img)\n",
        "  output = cv.connectedComponentsWithStats(threshold, 8, cv.CV_32S)\n",
        "  (numLabels, labels, stats, centroids) = output\n",
        "  digits = []\n",
        "  for i in range(numLabels):\n",
        "    # For each label/item, go over and get stats, then create rectangle and circles on image\n",
        "    x = stats[i, cv.CC_STAT_LEFT]\n",
        "    y = stats[i, cv.CC_STAT_TOP]\n",
        "    w = stats[i, cv.CC_STAT_WIDTH]\n",
        "    h = stats[i, cv.CC_STAT_HEIGHT]\n",
        "    area = stats[i, cv.CC_STAT_AREA]\n",
        "    (cX, cY) = centroids[i]\n",
        "    ratio = h/w\n",
        "    # X,Y Starting point - End point is top right of rectangle in this case where it is just x + width and y + height\n",
        "    if area >= 152 and area < 1000 and ratio >= 0.9:\n",
        "    #  digits.append(sign_img[y:y+h, x:x+w])\n",
        "      cv.rectangle(sign_img, (x, y), ((x + w), (y + h)), (0, 0, 255), 1)\n",
        "    #else:\n",
        "      #cv.putText(sign_img,str(area),(x,(y+10)), cv.FONT_HERSHEY_SIMPLEX, 0.4,(255,255,255),1,cv.LINE_AA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP_vFgoD1jxo"
      },
      "source": [
        "Task 1 Main Program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TgfASmPmdC5"
      },
      "outputs": [],
      "source": [
        "# Main Task 1 program that runs all required components\n",
        "def task_1(filenames):\n",
        "  for idx, i in enumerate(filenames):\n",
        "    img = cv.imread(i)\n",
        "    processed_img = image_process_set_1(img)\n",
        "    detected_sign = detect_number_sign(processed_img, img)\n",
        "\n",
        "    print(f\"Image {idx+1}:\")\n",
        "    get_digits(detected_sign)\n",
        "    cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5wwQr1nmoQe"
      },
      "source": [
        "Task 1 Training Results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBHrpLsXazPC"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/gdrive/MyDrive/MP/Assignment/BuildingSignageDetection/train' #Defining the path\n",
        "train_filenames = glob.glob(os.path.join(train_path, '*.jpg')) # image files with format png  in the folder\n",
        "task_1(train_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR1VfETqmv32"
      },
      "source": [
        "Task 1 Validation Results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k0cNsB7mvOO"
      },
      "outputs": [],
      "source": [
        "val_path = '/content/gdrive/MyDrive/MP/Assignment/BuildingSignageDetection/val' #Defining the path\n",
        "val_filenames = glob.glob(os.path.join(val_path, '*.jpg')) # image files with format png  in the folder\n",
        "task_1(val_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuGV4nfYyDN9"
      },
      "source": [
        "Task 1 Test Results: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yERx1ZnfyCdI"
      },
      "outputs": [],
      "source": [
        "test_path = '/content/gdrive/MyDrive/MP/Assignment/BuildingSignageDetection/test' #Defining the path\n",
        "test_filenames = glob.glob(os.path.join(test_path, '*.jpg')) # image files with format png  in the folder\n",
        "task_1(test_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck74bMgjYZNG"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Note: Please do not forget to run the 3 code blocks below to be able to run Task 2 code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnHjLSMn8JoG"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==0.17.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE--aqJC8MQU"
      },
      "outputs": [],
      "source": [
        "#Code to connect your google drive with google colaboratory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from d2l import torch as d2l\n",
        "\n",
        "import numpy as np \n",
        "from google.colab.patches import cv2_imshow \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u7uJrYm9DEA"
      },
      "source": [
        "Common methods for both approaches to train /evaluate models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP1XvVGzb_uJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"The following code has been retrived from https://d2l.ai/_modules/d2l/torch.html and has been modified\"\"\"\n",
        "\"\"\"Also taken and modified from my Machine Learning (COMP3010) assignment from last semester\"\"\"\n",
        "\n",
        "#Create trainer for training process (Used in train_model mathod)\n",
        "def create_trainer(net, learning_rate, param_group):\n",
        "  trainer = None #By default trainer is nothing\n",
        "  #If statement for different type of architectures since VGG and AlexNet do not have a fc component\n",
        "  if net.__class__.__name__ == \"ResNet\": #ResNet\n",
        "      if param_group: # If true, then the learning rate is 10 times greater for final layer\n",
        "          params_1x = [param for name, param in net.named_parameters()\n",
        "              if name not in [\"fc.weight\", \"fc.bias\"]]\n",
        "          trainer = torch.optim.SGD([{'params': params_1x},\n",
        "                                  {'params': net.fc.parameters(),\n",
        "                                      'lr': learning_rate * 10}],\n",
        "                                  lr=learning_rate, weight_decay=0.001)\n",
        "      else: # Uses current parameters for the trainer\n",
        "          trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
        "                                  weight_decay=0.001)\n",
        "  elif net.__class__.__name__ == \"VGG\" or net.__class__.__name__ == \"AlexNet\": #VGG or AlexNet\n",
        "      if param_group: # If true, then the learning rate is 10 times greater for final layer\n",
        "          params_1x = [param for name, param in net.named_parameters()\n",
        "              if name not in [\"classifier.6.weight\", \"classifier.6.bias\"]]\n",
        "          trainer = torch.optim.SGD([{'params': params_1x},\n",
        "                                  {'params': net.classifier[6].parameters(),\n",
        "                                      'lr': learning_rate * 10}],\n",
        "                                  lr=learning_rate, weight_decay=0.001)\n",
        "      else: # Uses current parameters for the trainer\n",
        "          trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
        "                                  weight_decay=0.001)\n",
        "  elif net.__class__.__name__ == \"NewModel\": #For Model without classifier\n",
        "      trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
        "                                  weight_decay=0.001)\n",
        "  return trainer\n",
        "\n",
        "\n",
        "# The purpose of the following method is to train the model and also evaluate val/test set\n",
        "def train_model(net, train_iter, val_iter, test_iter, loss, num_epochs, lr, \n",
        "                param_group, devices=d2l.try_all_gpus()):\n",
        "  \n",
        "  trainer = create_trainer(net, lr, param_group)\n",
        "  \n",
        "  \"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\n",
        "  timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "  animator = None\n",
        "  if test_iter: #If there is a test set\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
        "                          legend=['train loss', 'train acc', 'val acc', 'test acc'])\n",
        "  elif val_iter: #If there is only validation set\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
        "                          legend=['train loss', 'train acc', 'val acc'])\n",
        "  else: #If there is no validation or test set\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
        "                          legend=['train loss', 'train acc'])\n",
        "  net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "  for epoch in range(num_epochs):\n",
        "      # Sum of training loss, sum of training accuracy, no. of examples,\n",
        "      # no. of predictions\n",
        "      metric = d2l.Accumulator(4)\n",
        "      for i, (features, labels) in enumerate(train_iter):\n",
        "          timer.start()\n",
        "          l, acc = d2l.train_batch_ch13(\n",
        "              net, features, labels, loss, trainer, devices)\n",
        "          metric.add(l, acc, labels.shape[0], labels.numel())\n",
        "          timer.stop()\n",
        "          if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "            if not val_iter: #No validation set\n",
        "              animator.add(epoch + (i + 1) / num_batches,\n",
        "                            (metric[0] / metric[2], metric[1] / metric[3]))\n",
        "            elif test_iter: #Using test set\n",
        "              animator.add(epoch + (i + 1) / num_batches,\n",
        "                            (metric[0] / metric[2], metric[1] / metric[3],\n",
        "                            None, None))\n",
        "            else: #Only validation set\n",
        "              animator.add(epoch + (i + 1) / num_batches,\n",
        "                            (metric[0] / metric[2], metric[1] / metric[3],\n",
        "                            None))\n",
        "              \n",
        "      val_acc = None\n",
        "      test_acc = None\n",
        "\n",
        "      if test_iter: #If we are also doing evaluation on test set\n",
        "        val_acc = d2l.evaluate_accuracy_gpu(net, val_iter)\n",
        "        animator.add(epoch + 1, (None, None, val_acc, None))\n",
        "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
        "        animator.add(epoch + 1, (None, None, None, test_acc))\n",
        "      elif val_iter: #If we are doing evaluation only on validation set\n",
        "        val_acc = d2l.evaluate_accuracy_gpu(net, val_iter)\n",
        "        animator.add(epoch + 1, (None, None, val_acc))\n",
        "      \n",
        "  if test_iter: #If we are also doing evaluation on test set\n",
        "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
        "          f'{metric[1] / metric[3]:.3f}, val acc {val_acc:.3f}, test acc {test_acc:.3f}')\n",
        "  elif val_iter: #If no test set, but validation set\n",
        "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
        "          f'{metric[1] / metric[3]:.3f}, val acc {val_acc:.3f}')\n",
        "  else: #If only training set\n",
        "    print(f'loss {metric[0] / metric[2]:.3f}, train acc ' f'{metric[1] / metric[3]:.3f}')\n",
        "    \n",
        "  print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
        "          f'{str(devices)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd0izyRzlA-G"
      },
      "source": [
        "Data loading:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nbOKVLKY9C6"
      },
      "outputs": [],
      "source": [
        "#Params\n",
        "bs = 64 #Batch size\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transformations = transforms.Compose([\n",
        "    transforms.Resize(150),\n",
        "    transforms.CenterCrop(149),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "test_transformations = transforms.Compose([\n",
        "    transforms.CenterCrop(149),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "# training data loading\n",
        "train_dataset = datasets.ImageFolder(root='/content/gdrive/MyDrive/MP/Assignment/coral image classification/train', transform=transformations)  \n",
        "train_iter = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=2)\n",
        "\n",
        "# validation data loading\n",
        "valid_dataset = datasets.ImageFolder(root='/content/gdrive/MyDrive/MP/Assignment/coral image classification/val', transform=transformations)\n",
        "val_iter = DataLoader(valid_dataset, batch_size=bs, shuffle=False, num_workers=2)\n",
        "\n",
        "# testing data loading\n",
        "test_dataset = datasets.ImageFolder(root='/content/gdrive/MyDrive/MP/Assignment/coral image classification/test', transform=test_transformations)\n",
        "test_iter = DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irFleqgKRGGq"
      },
      "source": [
        "Common parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay56xpCPRCmL"
      },
      "outputs": [],
      "source": [
        "#Gets all available GPUs\n",
        "devices = d2l.try_all_gpus()\n",
        "criterion = nn.CrossEntropyLoss(reduction='mean') #Use cross entropy loss as this is a classification problem and it uses softmax by default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1JZhHZINFzB"
      },
      "source": [
        "### Approach 1 (Modern ML [Cross Entropy Loss + Softmax])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6lOKCd9-7jl"
      },
      "source": [
        "##### Determine which approach is better - Fine-tune pre-trained vs train from scratch (Using ResNet18 as base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaKPfpQ__OiB"
      },
      "outputs": [],
      "source": [
        "# Model Creation - Scratch AlexNet\n",
        "alexnet = torchvision.models.alexnet(pretrained=False)\n",
        "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 2)\n",
        "nn.init.xavier_uniform_(alexnet.classifier[6].weight);\n",
        "\n",
        "\"\"\" Train Model From Scratch \"\"\"\n",
        "train_model(alexnet, train_iter, val_iter, test_iter=None, loss=criterion, \n",
        "            num_epochs=5, lr=5e-2, param_group=False, devices=devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYHuG-aF_JbT"
      },
      "outputs": [],
      "source": [
        "# Model Creation - Pretrained AlexNet\n",
        "alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features, 2)\n",
        "nn.init.xavier_uniform_(alexnet.classifier[6].weight);\n",
        "\n",
        "\"\"\" Finetune Pretrained Model \"\"\"\n",
        "train_model(alexnet, train_iter, val_iter, test_iter=None, loss=criterion, \n",
        "            num_epochs=5, lr=6e-4, param_group=True, devices=devices) #5e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW1KvCOJ91de"
      },
      "source": [
        "##### Determine which model is the best out of AlexNet, VGG16 and ResNet18 (Using Pre-trained which is best from previous test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhEG5CJe__Qg"
      },
      "outputs": [],
      "source": [
        "# Model Creation - Pretrained VGG16\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
        "vgg16.fc = nn.Linear(vgg16.classifier[6].in_features, 2)\n",
        "nn.init.xavier_uniform_(vgg16.classifier[6].weight);\n",
        "\n",
        "\"\"\" Finetune Pretrained Model \"\"\"\n",
        "train_model(vgg16, train_iter, val_iter, test_iter=None, loss=criterion, \n",
        "            num_epochs=5, lr=3e-3, param_group=True, devices=devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7ltQI-fnwhv"
      },
      "outputs": [],
      "source": [
        "# Model Creation - Pretrained ResNet18\n",
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, 2)\n",
        "nn.init.xavier_uniform_(resnet18.fc.weight);\n",
        "\n",
        "\"\"\" Finetune Pretrained Model \"\"\"\n",
        "train_model(resnet18, train_iter, val_iter, test_iter=None, loss=criterion, \n",
        "            num_epochs=5, lr=1e-2, param_group=True, devices=devices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIeJYj6X_vd0"
      },
      "source": [
        "##### Best model train / evaluation (Pre-trained ResNet18) with test set (Once available)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj0n9VAk_9Sn"
      },
      "outputs": [],
      "source": [
        "# Model Creation - Pretrained ResNet18\n",
        "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, 2)\n",
        "nn.init.xavier_uniform_(resnet18.fc.weight);\n",
        "\n",
        "\"\"\" Finetune Pretrained Model \"\"\"\n",
        "train_model(resnet18, train_iter, val_iter, test_iter=test_iter, loss=criterion, \n",
        "            num_epochs=5, lr=1e-2, param_group=True, devices=devices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFQrAUZdNW4f"
      },
      "source": [
        "### Approach 2 (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl_h5TfN6eA5"
      },
      "source": [
        "##### Class to make new model from ResNet-18 (Feature extraction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sOsXtG96cPA"
      },
      "outputs": [],
      "source": [
        "\"\"\"The following code has been retrieved and slightly modified from my Machine Learning (COMP3010) Assignment\"\"\"\n",
        "\n",
        "# Class definition of a new model that extracts features specifically from ResNet and applies a classifier after the features\n",
        "class NewModel(nn.Module):\n",
        "  def __init__(self, pretrained_model):\n",
        "    super(NewModel, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.new_model = nn.Sequential(*list(pretrained_model.children())[:-1]) #Cuts the pretrained model to remove classifier or fc layer\n",
        "  \n",
        "  def forward(self, x): #The path of calls when training\n",
        "    x = self.new_model(x) # Input first goes through modified ResNet\n",
        "    features = self.flatten(x) # Applies flattening on the data\n",
        "    return features #Return flattened features for classifier to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfZSSR0l6fxi"
      },
      "source": [
        "##### Method to load entire dataset into memory to be used for SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb6uOUMNxsrD"
      },
      "outputs": [],
      "source": [
        "#Retrieves all images and their corresponding labels in the same order\n",
        "def load_dataset_into_memory(dataset): \n",
        "  set_X = list()\n",
        "  set_Y = list()\n",
        "  for i in range(len(dataset)): \n",
        "      img, label = dataset[i]\n",
        "      set_X.append(img)\n",
        "      set_Y.append(label)\n",
        "  set_X = torch.stack(set_X)\n",
        "  set_Y = torch.IntTensor(set_Y)\n",
        "  return set_X, set_Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DVBV-YNPUj_"
      },
      "source": [
        "##### SVM Methods for fitting and classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sFcB0TbPTsZ"
      },
      "outputs": [],
      "source": [
        "# Method to fit the SVM classifier on the extracted training dataset features\n",
        "def SVM_fitting():\n",
        "  #Training dataset loading and features extraction\n",
        "  train_X, train_Y = load_dataset_into_memory(train_dataset)\n",
        "  train_features = new_net(train_X.to('cuda'))\n",
        "  train_features = train_features.cpu()\n",
        "  del train_X\n",
        "\n",
        "  #Fitting SVM classifier on extracted features from training dataset \n",
        "  with torch.no_grad(): # No grad is temporary on so it doesn't store the graph or variables for computing gradients\n",
        "    svm_classifier.fit(train_features, train_Y)\n",
        "  del train_features\n",
        "  del train_Y\n",
        "\n",
        "#Method to use SVM as a classifier on exctracted validation / testing set features\n",
        "def SVM_classification(test_flag):\n",
        "  #Validation dataset loading and features extraction\n",
        "  val_X, val_Y = load_dataset_into_memory(valid_dataset)\n",
        "  val_features = new_net(val_X.to('cuda'))\n",
        "  val_features = val_features.cpu()\n",
        "  del val_X\n",
        "\n",
        "  #Prediction using SVM classifier\n",
        "  with torch.no_grad(): # No grad is temporary on so it doesn't store the graph or variables for computing gradients\n",
        "    val_predictions = svm_classifier.predict(val_features)\n",
        "  del val_features\n",
        "\n",
        "  # Accuracy on Validation\n",
        "  print(\"Validation Accuracy:\", accuracy_score(val_Y, val_predictions)) # Calculate score of trained classifer comparing with validation run and validation labels\n",
        "  del val_Y\n",
        "\n",
        "  if test_flag:\n",
        "    test_X, test_Y = load_dataset_into_memory(test_dataset)\n",
        "    test_features = new_net(test_X.to('cuda'))\n",
        "    test_features = test_features.cpu()\n",
        "    del test_X\n",
        "\n",
        "    #Prediction using SVM classifier\n",
        "    with torch.no_grad(): # No grad is temporary on so it doesn't store the graph or variables for computing gradients\n",
        "      test_predictions = svm_classifier.predict(test_features)\n",
        "    del test_features\n",
        "\n",
        "    # Accuracy on Test\n",
        "    print(\"Test Accuracy:\", accuracy_score(test_Y, test_predictions)) # Calculate score of trained classifer comparing with test run and test labels\n",
        "    del test_Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjD0pKSrC-rV"
      },
      "source": [
        "##### SVM Classification on fine-tuned pre-trained ResNet18\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LD0dj5VGu0v9"
      },
      "outputs": [],
      "source": [
        "res_net = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "new_net = NewModel(res_net).to(device) # Removes classifier layer (Last CNN)\n",
        "#No need for validation set to be evaluated as we do not have a classifier\n",
        "train_model(new_net, train_iter, val_iter=None, test_iter=None, loss=criterion, \n",
        "            num_epochs=5, lr=1e-1, param_group=None, devices=devices) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp0EZZPlkpcq"
      },
      "outputs": [],
      "source": [
        "svm_classifier = SVC(kernel='linear')\n",
        "SVM_fitting()\n",
        "SVM_classification(test_flag=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "952KupTaruaC"
      },
      "outputs": [],
      "source": [
        "svm_classifier = SVC(kernel='rbf')\n",
        "SVM_fitting()\n",
        "SVM_classification(test_flag=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx_1T2hquC3S"
      },
      "outputs": [],
      "source": [
        "svm_classifier = SVC(kernel='poly')\n",
        "SVM_fitting()\n",
        "SVM_classification(test_flag=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POGlR0I3xe1V"
      },
      "source": [
        "##### SVM Classification on non-fine-tuned pre-trained ResNet18\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDMWr17cxI7b"
      },
      "outputs": [],
      "source": [
        "res_net = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "new_net = NewModel(res_net).to(device) # Removes classifier layer (Last CNN)\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "SVM_fitting()\n",
        "SVM_classification(test_flag=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-ZQU5yjsP38"
      },
      "outputs": [],
      "source": [
        "res_net = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "new_net = NewModel(res_net).to(device) # Removes classifier layer (Last CNN)\n",
        "svm_classifier = SVC(kernel='rbf')\n",
        "SVM_fitting()\n",
        "SVM_classification(test_flag=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_UnxG8Btt_h"
      },
      "outputs": [],
      "source": [
        "res_net = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "new_net = NewModel(res_net).to(device) # Removes classifier layer (Last CNN)\n",
        "svm_classifier = SVC(kernel='poly')\n",
        "SVM_fitting()\n",
        "SVM_classification(test_flag=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8y_Q2o3zTzo"
      },
      "source": [
        "##### Evaluation of best method with validation and test set (Once available)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLcvkbyxzaxe"
      },
      "outputs": [],
      "source": [
        "res_net = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "new_net = NewModel(res_net).to(device) # Removes classifier layer (Last CNN)\n",
        "#No need for validation set to be evaluated as we do not have a classifier\n",
        "train_model(new_net, train_iter, val_iter=None, test_iter=None, loss=criterion, \n",
        "            num_epochs=5, lr=1e-1, param_group=None, devices=devices) \n",
        "\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "SVM_fitting()\n",
        "SVM_classification(test_flag=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu5_U9CFm6My"
      },
      "source": [
        "### References for all code adapted / used in this notebook \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrcgQZGpnEa1"
      },
      "source": [
        "Maganha, Felipe. 2022. “Detecting handwriting regions with openCV and Python”. Accessed September 21, 2022. https://felipemeganha.medium.com/detecting-handwriting-regions-with-opencv-and-python-ff0b1050aa4e <br>\n",
        " \n",
        "OpenCV. n.d. “OpenCV: Image Thresholding”. Accessed September 20, 2022. https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html. <br>\n",
        " \n",
        "OpenCV. n.d. “OpenCV: cv::MSER Class Reference”. Accessed September 21, 2022. https://docs.opencv.org/4.x/d3/d28/classcv_1_1MSER.html. <br>\n",
        " \n",
        "Outram Harrison. 2022. COMP3010 - Tutorial 7. Youtube video, 53:27. https://www.youtube.com/watch?v=NVcJhMtRErE.<br>\n",
        " \n",
        "Outram Harrison. 2022. “Tutorial 7 – Extracting features out of networks” Curtin University. Accessed May 6, 2022. https://colab.research.google.com/drive/1WHbHe9I_AQo1W_bdZOr9fP4cGb6543d7. <br>\n",
        " \n",
        "PyTorch Team. n.d. “RESNET” PyTorch. Accessed May 3, 2022. https://pytorch.org/hub/pytorch_vision_resnet/. <br>\n",
        " \n",
        "PyTorch Team. 2017. “TORCHVISION.MODELS” PyTorch. Accessed May 3, 2022. https://pytorch.org/vision/0.8/models.html. <br>\n",
        " \n",
        "PyTorch Team. 2022. “TRAINING A CLASSIFIER” PyTorch. Accessed May 3, 2022. https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html. <br>\n",
        " \n",
        "Zhang, Aston, Lipton, C., Zachary. Li, Mu, & Smola, Alexander J. 2020. “Source code for d2l.torch” Dive into Deep Learning. Accessed April 30, 2022. https://d2l.ai/_modules/d2l/torch.html. <br>\n",
        " \n",
        "Zhang, Aston, Lipton, C., Zachary. Li, Mu, & Smola, Alexander J. 2020. “13.11. Fully Convolutional Networks” Dive into Deep Learning. Accessed May 2, 2022. https://d2l.ai/chapter_computer-vision/fcn.html.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSFeUCNTebTC"
      },
      "source": [
        "### Extra Testing Code (Not used)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1RJUc0weZX7"
      },
      "outputs": [],
      "source": [
        "#Code to connect your google drive with google colaboratory\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import cv2 as cv #openCV is a key library that provides various useful functions for computer vision\n",
        "import os\n",
        "from skimage import io, transform, data\n",
        "import numpy as np #Numpy library provides various useful functions and operators for scientific computing\n",
        "from google.colab.patches import cv2_imshow \n",
        "from sklearn import svm\n",
        "\n",
        "#Image transformations (Resize then center crop)\n",
        "def transforms(img, dim_resize=150, dim_crop=149):\n",
        "  #Resize\n",
        "  img_resize = cv.resize(img, (dim_resize,dim_resize), interpolation = cv.INTER_AREA)\n",
        "\n",
        "  #Center Crop [Retrieved from: https://stackoverflow.com/questions/61927877/how-to-crop-opencv-image-from-center]\n",
        "  center = img_resize.shape\n",
        "  x = center[1]/2 - dim_crop/2\n",
        "  y = center[0]/2 - dim_crop/2\n",
        "  img_crop = img_resize[int(y):int(y+dim_crop), int(x):int(x+dim_crop)]\n",
        "  return img_crop\n",
        "\n",
        "# Data loading retrieved and modified from https://kapernikov.com/tutorial-image-classification-with-scikit-learn/\n",
        "def load_with_transforms(val_flag, dim_resize=150, dim_crop=149):\n",
        "    data = dict()\n",
        "    data['label'] = []\n",
        "    data['filename'] = []\n",
        "    data['data'] = []   \n",
        "\n",
        "    path = '/content/gdrive/MyDrive/MP/Assignment/coral image classification/val' if val_flag else '/content/gdrive/MyDrive/MP/Assignment/coral image classification/train'\n",
        "    # read all images in PATH, resize and write to DESTINATION_PATH\n",
        "    for subdir in os.listdir(path):\n",
        "      current_path = os.path.join(path, subdir)\n",
        " \n",
        "      for file in os.listdir(current_path):\n",
        "        if file[-3:] in {'jpg', 'png'}:\n",
        "            img = cv.imread(os.path.join(current_path, file))\n",
        "            img = cv.cvtColor(img, cv.COLOR_RGB2RGBA)\n",
        "            img = transforms(img, dim_resize, dim_crop)\n",
        "            data['label'].append(subdir[:-4])\n",
        "            data['filename'].append(file)\n",
        "            data['data'].append(img)\n",
        "            #print(img.shape)\n",
        "    X = np.array(data['data'])\n",
        "    Y = np.array(data['label'])\n",
        "    return X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmnjeduWvRgo"
      },
      "outputs": [],
      "source": [
        "train_X, train_Y = load_with_transforms(val_flag=False, dim_resize=150, dim_crop=149)\n",
        "svm_model = svm.SVC()\n",
        "svm_model.fit(train_X, train_Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aqAQOCnvPKG"
      },
      "outputs": [],
      "source": [
        "val_X, val_Y = load_with_transforms(val_flag=False, dim_resize=150, dim_crop=149)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hSFeUCNTebTC"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e35c5856cff72d20e6e0e19445c0d45b03888a7b19367444be448131dd51e693"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
